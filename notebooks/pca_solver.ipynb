{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c6ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial.distance import cdist, pdist, squareform, euclidean\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "from sklearn.preprocessing import normalize\n",
    "from gensim import models\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bedd1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_goog_file(data_dir: str='.', size: int=None, clean=False) -> Tuple[List[str], np.array]:\n",
    "    \"\"\"\n",
    "    Read the 3M vectors of length 300 from the Google News dataset. \n",
    "    \n",
    "    Returns a list of words and a matrix of vectors. Indices match between the two.\n",
    "    The vectors are normalized to unit, because Semantle uses cos distance which ignores magnitude.\n",
    "    \n",
    "    data_dir: dir that contains source files\n",
    "    'size' and 'clean' will produce fewer rows.\n",
    "    size: a number smaller than 3 million. Stops reading once we hit that many rows.\n",
    "    clean: only keep high-quality words (no spaces, numbers, capital letters, or non-ascii.)\n",
    "    \"\"\"\n",
    "    vec_file = os.path.join(data_dir + 'GoogleNews-vectors-negative300.bin')\n",
    "    kv = models.KeyedVectors.load_word2vec_format(vec_file, binary=True, limit=size)\n",
    "    words = kv.index_to_key\n",
    "    vecs = []\n",
    "    w_list = []\n",
    "    \n",
    "    # subset setup\n",
    "    if clean:\n",
    "        ascii_subset = re.compile(r'^[a-z]+$')\n",
    "\n",
    "    # iterate over file\n",
    "    for w in words:\n",
    "        if clean:\n",
    "            # filter out proper nouns, pictograms, emoji, multi-words, etc. Eliminates 95% of the dataset.   \n",
    "            if not ascii_subset.match(w):\n",
    "                continue\n",
    "        w_list.append(w)\n",
    "        vecs.append(kv[w])\n",
    "    \n",
    "    # combine, normalize, return\n",
    "    mat = np.vstack(vecs)\n",
    "    return w_list, normalize(mat, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e10c6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_list size: 3000000 mat shape: (3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/mnt/Spookley/datasets/semantle/'\n",
    "w_list, mat_orig = read_goog_file(data_dir=data_dir, clean=False)\n",
    "print('w_list size:', len(w_list), 'mat shape:', mat_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1d8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b83e0956",
   "metadata": {},
   "source": [
    "### core semantle-solver stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac21fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_point_in_dist(point, dist):\n",
    "    # For when we know the dist but have no idea what direction to travel\n",
    "    vec = np.random.random((len(point)))\n",
    "    vec = vec / scipy.linalg.norm(vec)\n",
    "    vec = vec * dist*0.5\n",
    "    return vec+point\n",
    "\n",
    "\n",
    "def directed_point_in_dist(p1, p2, p1_dist, p2_dist):\n",
    "    # Generate a vector using p1 and p2.\n",
    "    # Check if it will point in the general direction of our target.\n",
    "    p1p2 = (p1-p2)\n",
    "    p1p2mag = scipy.linalg.norm(p1p2)\n",
    "    if p1p2mag < 0.0000001:\n",
    "        return None, 0\n",
    "    p1p2_unit = p1p2 / p1p2mag\n",
    "    if p1_dist < p2_dist:\n",
    "        # p1 is closer to target\n",
    "        mag = p1_dist\n",
    "        target_point = p1 + p1p2_unit*mag\n",
    "        confidence = (p2_dist-p1_dist) / p1p2mag\n",
    "        assert confidence >= 0\n",
    "    else:\n",
    "        # j is closer to target\n",
    "        # make a vector from j to a target that is dists[j] away\n",
    "        mag = p2_dist\n",
    "        target_point = p2 - p1p2_unit*mag\n",
    "        confidence = (p1_dist-p2_dist) / p1p2mag\n",
    "        assert confidence >= 0\n",
    "    return target_point, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa670d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_dist(score: float, curve: np.array):\n",
    "    c_dist = 1 - (score / 100)\n",
    "    polyfunc = np.poly1d(curve)\n",
    "    return polyfunc(c_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb32f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class for looking up word vectors in original or reduced space.\n",
    "Used by both Game and Player classes.\n",
    "\"\"\"\n",
    "@dataclasses.dataclass\n",
    "class WordInfo:\n",
    "    mat_orig: np.array\n",
    "    mat_reduced: np.array\n",
    "    curve: np.array\n",
    "    word_to_idx: Dict[str, int]\n",
    "    w_list: List[str]\n",
    "    ann_index: AnnoyIndex\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdafa949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(w_list: List[str], mat_reduced: np.array) -> AnnoyIndex:    \n",
    "    ann_index = AnnoyIndex(mat_reduced.shape[1], 'euclidean')\n",
    "    with tqdm(total=len(w_list)) as pbar:\n",
    "        for i, w in enumerate(w_list):\n",
    "            pbar.update(1)\n",
    "            ann_index.add_item(i, mat_reduced[i, :])\n",
    "    ann_index.build(20) # n trees\n",
    "    return ann_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3587689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemantleGame():\n",
    "    def __init__(self, info: WordInfo):\n",
    "        self.target_word = random.choice(info.w_list)\n",
    "        idx = info.word_to_idx[self.target_word]\n",
    "        self.target_vec = info.mat_orig[idx, :]\n",
    "        \n",
    "    def guess(self, word, info: WordInfo) -> Tuple[bool, float]:\n",
    "        # construct guess\n",
    "        idx = info.word_to_idx[word]\n",
    "        vec = info.mat_orig[idx, :]\n",
    "        semantle_score = (1-cos_dist(vec, self.target_vec))*100\n",
    "        if word == self.target_word:\n",
    "            return True, semantle_score\n",
    "        else:\n",
    "            return False, semantle_score\n",
    "    \n",
    "    def display_guesses(self):\n",
    "        s = []\n",
    "        for g in sorted(self.guesses, key = lambda g: g.dist):\n",
    "            s.append(str(g))\n",
    "        print('\\n'.join(s))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '\\n'.join('{}: {}'.format(k, v) for k, v in self.__dict__.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501c3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Guess:\n",
    "    word: str\n",
    "    num: int\n",
    "    dist: float\n",
    "    score: float\n",
    "    \n",
    "class SemantleSolver:\n",
    "    \n",
    "    def __init__(self, info: WordInfo, \n",
    "                 n_random_guesses=5, conf_thresh=0.1):\n",
    "        \n",
    "        self.n_random_guesses = n_random_guesses\n",
    "        self.conf_thresh = conf_thresh\n",
    "        \n",
    "        self.guesses = []  # List[Guess]\n",
    "        self.guessed_words = set()  # for fast lookup\n",
    "        self.best_guess = None\n",
    "        self.closest_dist = float('inf')\n",
    "        \n",
    "        self.stats = {\n",
    "            'grd_high_conf': 0,\n",
    "            'grd_random_dist': 0,\n",
    "            'times_gradient': 0,\n",
    "            'times_exhaustive': 0,\n",
    "            'times_random': 0,\n",
    "        }\n",
    "        \n",
    "    def _gradient_method(self, info: WordInfo):\n",
    "        # Use gradient method to get a closer guess.\n",
    "        w1 = self.guesses[-1].word\n",
    "        idx1 = info.word_to_idx[w1]\n",
    "        p1 = info.mat_reduced[idx1, :]\n",
    "        p1_dist = self.guesses[-1].dist\n",
    "        \n",
    "        # Consider the few most recent points. \n",
    "        # Try and find one with a vector through p1 that points towards the target.\n",
    "        best_point = None\n",
    "        best_confidence = 0\n",
    "        best_p2_dist = float('inf')\n",
    "        for i in range(2, min(10, len(self.guesses))):\n",
    "            w2 = self.guesses[-i].word\n",
    "            idx2 = info.word_to_idx[w2]\n",
    "            p2 = info.mat_reduced[idx2, :]\n",
    "            p2_dist = self.guesses[-i].dist\n",
    "            \n",
    "            # where does p2->p1 point? and how well aligned is that spot with the target?\n",
    "            target_point, confidence = directed_point_in_dist(p1, p2, p1_dist, p2_dist)\n",
    "            if confidence > best_confidence:\n",
    "                best_confidence = confidence\n",
    "                best_point = target_point\n",
    "        if best_confidence < self.conf_thresh:\n",
    "            # Low confidence in all computed directions. Let's pick a random direction and hope \n",
    "            # it gives us some new information.\n",
    "            self.stats['grd_random_dist'] += 1\n",
    "            idx_best = info.word_to_idx[self.best_guess]\n",
    "            vec = np.array(info.mat_reduced[idx_best, :])\n",
    "            best_point = random_point_in_dist(vec, self.closest_dist)\n",
    "        else:\n",
    "            self.stats['grd_high_conf'] += 1\n",
    "\n",
    "        return best_point\n",
    "\n",
    "    \n",
    "    def find_next_guess(self, info: WordInfo) -> bool:\n",
    "        if len(self.guesses) < self.n_random_guesses:\n",
    "            self.stats['times_random'] += 1\n",
    "            next_word = random.choice(info.w_list)\n",
    "        else:\n",
    "            self.stats['times_gradient'] += 1\n",
    "            v = self._gradient_method(info)\n",
    "            idxs_near_best = info.ann_index.get_nns_by_vector(v, 1000)\n",
    "            for idx in idxs_near_best:\n",
    "                w = info.w_list[idx]\n",
    "                if w not in self.guessed_words:\n",
    "                    next_word = w\n",
    "                    break\n",
    "            if next_word is None:\n",
    "                # If we've guessed 1000 points near this target already, just\n",
    "                # guess anything. If we hit this point, probably this solver param set is trash.\n",
    "                next_word = random.choice(info.w_list)\n",
    "            \n",
    "        return next_word\n",
    "\n",
    "    def make_guess(self, word, info: WordInfo, game: SemantleGame):\n",
    "        # guess the word\n",
    "        win, score = game.guess(word, info)\n",
    "        dist = score_to_dist(score, info.curve)\n",
    "        self.guessed_words.add(word)\n",
    "        self.guesses.append(Guess(word=word, dist=dist, num=len(self.guesses)+1, score=score))\n",
    "        \n",
    "        # see if this one's better\n",
    "        if self.best_guess is None or dist < self.closest_dist:\n",
    "            #print(word, round(dist, 3))\n",
    "            self.closest_dist = dist\n",
    "            self.best_guess = word\n",
    "        \n",
    "        if win:\n",
    "            #print(\"I win!\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def add_guess(self, word, score):\n",
    "        # Adds a guess from an external source. For playing Real Semantle.\n",
    "        dist = score_to_dist(score, self.curve)\n",
    "        self.guessed_words.add(word)\n",
    "        self.guesses.append(Guess(word=word, dist=dist, num=len(self.guesses)+1, score=score))\n",
    "        if self.best_guess is None or dist < self.closest_dist:\n",
    "            #print(word, round(dist, 3))\n",
    "            self.closest_dist = dist\n",
    "            self.best_guess = word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d18cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ddca8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9ff77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606e4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70a2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solver(info: WordInfo):\n",
    "    game = SemantleGame(info)\n",
    "    player = SemantleSolver(info, n_random_guesses=10, conf_thresh=0.1)\n",
    "    \n",
    "    won = False\n",
    "    while not won:\n",
    "        word = player.find_next_guess(info)\n",
    "        won = player.make_guess(word, info, game)\n",
    "        g = player.guesses[-1]\n",
    "        local_dist = score_to_dist(g.score, curve)\n",
    "        #print(g.word, round(g.score, 3), '->', round(local_dist, 3))\n",
    "        if len(player.guesses) >= 1000:\n",
    "            #print('stopped. ')\n",
    "            #print('Best guess:', player.best_guess, 'dist:', player.closest_dist)\n",
    "            break\n",
    "    print(player.stats)\n",
    "\n",
    "    max_score = 0\n",
    "    traj = []\n",
    "    for g in player.guesses:\n",
    "        max_score = max(g.score, max_score)\n",
    "        traj.append(max_score)\n",
    "        \n",
    "    plt.plot(range(len(traj)), traj, '-')\n",
    "    plt.xlim(0, 1000)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.title('PCA trajectories, n_dims={}, n_rows={}'.format(n_dims, len(info.w_list)))\n",
    "    plt.ylabel('Max score so far, 100=win')\n",
    "    plt.xlabel('Guess number, 1000=lose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602ddf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4bddf58a35439ebc3b737a970fc176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grd_high_conf': 895, 'grd_random_dist': 95, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 893, 'grd_random_dist': 97, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 980, 'grd_random_dist': 10, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 360, 'grd_random_dist': 13, 'times_gradient': 373, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 950, 'grd_random_dist': 40, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 919, 'grd_random_dist': 71, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 846, 'grd_random_dist': 144, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 950, 'grd_random_dist': 40, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 822, 'grd_random_dist': 168, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 933, 'grd_random_dist': 57, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e59213fca43424fae2a8b02dfed3f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grd_high_conf': 887, 'grd_random_dist': 69, 'times_gradient': 956, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 896, 'grd_random_dist': 94, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 973, 'grd_random_dist': 17, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 911, 'grd_random_dist': 79, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 847, 'grd_random_dist': 143, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 121, 'grd_random_dist': 7, 'times_gradient': 128, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 953, 'grd_random_dist': 37, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 887, 'grd_random_dist': 103, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 928, 'grd_random_dist': 62, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 953, 'grd_random_dist': 37, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2e90716c33447ab1a90fe6b2deb44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grd_high_conf': 304, 'grd_random_dist': 21, 'times_gradient': 325, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 511, 'grd_random_dist': 126, 'times_gradient': 637, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 175, 'grd_random_dist': 8, 'times_gradient': 183, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 314, 'grd_random_dist': 38, 'times_gradient': 352, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 325, 'grd_random_dist': 58, 'times_gradient': 383, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 173, 'grd_random_dist': 1, 'times_gradient': 174, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 141, 'grd_random_dist': 13, 'times_gradient': 154, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 930, 'grd_random_dist': 60, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 912, 'grd_random_dist': 78, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 368, 'grd_random_dist': 52, 'times_gradient': 420, 'times_exhaustive': 0, 'times_random': 10}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783413160cfc43abba6c4a44785ae445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grd_high_conf': 195, 'grd_random_dist': 16, 'times_gradient': 211, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 793, 'grd_random_dist': 163, 'times_gradient': 956, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 880, 'grd_random_dist': 72, 'times_gradient': 952, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 608, 'grd_random_dist': 86, 'times_gradient': 694, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 506, 'grd_random_dist': 220, 'times_gradient': 726, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 576, 'grd_random_dist': 102, 'times_gradient': 678, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 511, 'grd_random_dist': 40, 'times_gradient': 551, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 886, 'grd_random_dist': 104, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 716, 'grd_random_dist': 274, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 593, 'grd_random_dist': 69, 'times_gradient': 662, 'times_exhaustive': 0, 'times_random': 10}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98875231fe84484fbef8895c1804487f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grd_high_conf': 473, 'grd_random_dist': 99, 'times_gradient': 572, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 102, 'grd_random_dist': 26, 'times_gradient': 128, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 141, 'grd_random_dist': 18, 'times_gradient': 159, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 100, 'grd_random_dist': 20, 'times_gradient': 120, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 201, 'grd_random_dist': 38, 'times_gradient': 239, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 72, 'grd_random_dist': 3, 'times_gradient': 75, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 96, 'grd_random_dist': 10, 'times_gradient': 106, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 443, 'grd_random_dist': 54, 'times_gradient': 497, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 470, 'grd_random_dist': 74, 'times_gradient': 544, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 147, 'grd_random_dist': 9, 'times_gradient': 156, 'times_exhaustive': 0, 'times_random': 10}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fde59073ac44f39e0b5042812fb658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grd_high_conf': 622, 'grd_random_dist': 368, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 590, 'grd_random_dist': 122, 'times_gradient': 712, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 277, 'grd_random_dist': 84, 'times_gradient': 361, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 896, 'grd_random_dist': 94, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 375, 'grd_random_dist': 103, 'times_gradient': 478, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 710, 'grd_random_dist': 280, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 732, 'grd_random_dist': 258, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 370, 'grd_random_dist': 103, 'times_gradient': 473, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 535, 'grd_random_dist': 455, 'times_gradient': 990, 'times_exhaustive': 0, 'times_random': 10}\n",
      "{'grd_high_conf': 436, 'grd_random_dist': 209, 'times_gradient': 645, 'times_exhaustive': 0, 'times_random': 10}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e7d6a2c13a498990af22cd25a40a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in each PCA matrix and its curve\n",
    "# build an ANN index on that PCA\n",
    "# then play a bunch of games to see how well it does\n",
    "n_rows = len(w_list)\n",
    "pf_file = os.path.join(data_dir, 'pca_curve_fits_{}rows.pkl'.format(n_rows))\n",
    "with open(pf_file, 'rb') as fh:\n",
    "    curves = pickle.load(fh)\n",
    "    \n",
    "for n_dims in [10, 25, 50, 75, 100, 200, 300]:\n",
    "    fn = os.path.join(data_dir, 'pca_{}rows_{}dims.npy'.format(n_rows, n_dims))\n",
    "    curve = curves[n_dims]\n",
    "    mat_pca = np.load(fn)\n",
    "    \n",
    "    ann_index = build_index(w_list, mat_pca)\n",
    "    \n",
    "    info = WordInfo(\n",
    "        mat_orig = mat_orig,\n",
    "        mat_reduced = mat_pca,\n",
    "        curve = curve,\n",
    "        word_to_idx = {w:idx for idx, w in enumerate(w_list)},\n",
    "        w_list = w_list,\n",
    "        ann_index = ann_index\n",
    "    )\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.title('PCA solver, {} dims {} rows'.format(n_dims, n_rows))\n",
    "    for _ in range(10):\n",
    "        run_solver(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48c071",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d0dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
